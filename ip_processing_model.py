# -*- coding: utf-8 -*-
"""IP_processing_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19313emups6zpP2UAPRE3thpM8WQNja1j
"""

from google.colab import drive
drive.mount('/content/drive')

import sys, subprocess
import os
import wave
from scipy.io import wavfile
import matplotlib.pyplot as plt
import noisereduce as nr
from keras.preprocessing import image
from keras.utils.np_utils import to_categorical
import tensorflow as tf
import io
import numpy as np
import keras
from tensorflow.python.ops.numpy_ops import np_config
from keras.models import Sequential
from keras import optimizers
from keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization
from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback
from keras.utils.np_utils import to_categorical

colab_requirements = [
    "pip install librosa",
    "pip install noisereduce",
    "pip install soundfile",
    "pip install pydub",
    "pip install control",
    "pip install wave",
    "pip install wavio",
    "pip install tensorboardcolab"
]
def run_subprocess_command(cmd):
    # run the command
    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)
    # print the output
    for line in process.stdout:
        print(line.decode().strip())
IN_COLAB = "google.colab" in sys.modules
if IN_COLAB:
    for i in colab_requirements:
        run_subprocess_command(i)

try:
    os.mkdir("/content/drive/MyDrive/Output_noise_reduced")
except:
    print("Folder already found")

rate=[]
data=[]
path = "/content/drive/MyDrive/voice-data"
  
# Change the directory
os.chdir(path)        
files=[]  
# iterate through all file
for file in os.listdir():
    # Check whether file is in text format or not
    if file.endswith(".wav"):
      rate1, data1 = wavfile.read(f"{path}/{file}")
      rate.append(rate1)
      data.append(data1)
      files.append(file)



#To use wavio, do install wavio by - pip install wavio
from scipy.io.wavfile import write
import wavio as wv
reduced_noise=[]

for i in range(len(data)):
  rn=nr.reduce_noise(y = data[i], sr=rate[i], n_std_thresh_stationary=1,stationary=True)
  reduced_noise.append(rn)
  sound_name = f"{files[i]}"
  path = "/content/drive/MyDrive/Output_noise_reduced"
  wv.write(os.path.join(path,sound_name),reduced_noise[i],rate=rate[i])

FOLDER_PATH = 'Output_noise_reduced'
ROOT_PATH = '/content/drive/MyDrive/'
print(len(os.listdir(os.path.join(ROOT_PATH, FOLDER_PATH))))

np_config.enable_numpy_behavior()

path = "/content/drive/MyDrive/Output_noise_reduced/"

Xs=[]
Ys=[]
log_specs=[]
# Change the directory
os.chdir(path) 
i=0
#os.mkdir("/content/drive/MyDrive/spec_images/")
# iterate through all file
for file in os.listdir():
  print(file)
  if '.ipynb' not in file:
    ratechunk, datachunk = wavfile.read(f"{path}/{file}")
    reduced_noise = tf.cast(datachunk, dtype=tf.float32)
    spectrogram = tf.signal.stft(reduced_noise, frame_length=255, frame_step=128)
    spectrogram = tf.abs(spectrogram)
    spectogram=spectrogram.numpy()
    log_specs.append(np.log(spectrogram.T + np.finfo(float).eps))
    height = log_specs[i].shape[0]
    width = log_specs[i].shape[1]
    Xs.append(np.linspace(0, np.size(spectrogram), num=width, dtype=int))
    Ys.append(range(height))
    i=i+1
    
   

fig=[]
j=0
for i in range(len(log_specs)):
  spectogram_dimensions=(3,3)
  fig.append(plt.figure())
  fig[i].set_size_inches((spectogram_dimensions[0],spectogram_dimensions[1]))
  ax=plt.Axes(fig[i],[0.,0.,1.,1.])
  ax.pcolormesh(Xs[i], Ys[i], log_specs[i])
  fig[i].add_axes(ax)
  #ax.specgram(reduced_noise,cmap='Blues',Fs=2,noverlap=16)
  #ax.xaxis.set_major_locator(plt.NullLocator())
  #ax.yaxis.set_major_locator(plt.NullLocator())
  fig[i].savefig("/content/drive/MyDrive/spec_images/"+files[i][:3]+".png",pad_inches=1)

imagesDir = "/content/drive/MyDrive/spec_images/"
trainset = []
testset = []
for file in os.listdir(imagesDir):
  if '.ipynb' not in file:
    label = file.split('_')[1][:-4]
    sample_number = file.split('_')[0]
    print(sample_number,label)
    img = image.load_img(imagesDir+file)
    if sample_number in ['1','3']:  
      testset.append([image.img_to_array(img), label])
    else:
      trainset.append([image.img_to_array(img), label])
#print(trainset)
#print(testset)

# Get only images in the train list not the Labels
X_train = [item[0] for item in trainset]

# Get only Labels in the train list not the images
y_train = [item[1] for item in trainset]
print(y_train)
# Get only images in the test list not the Labels
X_test = [item[0] for item in testset]

# Get only Labels in the test list not the images
y_test = [item[1] for item in testset]
print(y_test)
print(X_test)

X_train = np.asanyarray(X_train)
y_train = np.asanyarray(y_train)
X_test = np.asanyarray(X_test)
y_test = np.asanyarray(y_test)

# converting y data into categorical (one-hot encoding)
print(y_train)
y_train = to_categorical(y_train)
y_test = to_categorical(y_test)
print(y_train)

os.chdir("/content/drive/MyDrive") 
np.save('xtrain_file', X_train)
np.save('xtest_file', X_test)
np.save('ytrain_file', y_train)
np.save('ytest_file', y_test)

#Loading the numpy array file 
X_train=np.load('/content/drive/MyDrive/xtrain_file.npy')
X_test=np.load('/content/drive/MyDrive/xtest_file.npy')
Y_train=np.load('/content/drive/MyDrive/ytrain_file.npy')
Y_test=np.load('/content/drive/MyDrive/ytest_file.npy')

print(len(X_train))
print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)

data_shape = (X_train.shape[1], X_train.shape[2], X_train.shape[3])

from keras.layers import LSTM
from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.metrics import accuracy_score
X_train1 = X_train.reshape((X_train.shape[0],X_train.shape[1], X_train.shape[2] * X_train.shape[3]))
X_test1 = X_test.reshape((X_test.shape[0],X_test.shape[1], X_test.shape[2] * X_test.shape[3]))
print(X_test1.shape)

def cnn_1():
    model = Sequential()
    
    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=data_shape))
    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dense(5, activation='softmax'))
  
    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    
    return model

def cnn_2():
    model = Sequential()
    
    model.add(Conv2D(128, (3, 3), padding='same',activation = 'relu', input_shape=data_shape)) 
    model.add(MaxPooling2D(pool_size=(2, 2)))
    
    model.add(Conv2D(128, (3, 3), padding='same'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    
    model.add(Conv2D(64, (3, 3), padding='same'))
    model.add(Activation('relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    
    
    model.add(Flatten())
    
    model.add(Dense(64,activation = 'relu'))
    
    model.add(Dense(32,activation = 'relu'))
  

    model.add(Dense(5, activation='softmax'))
    
    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy',tf.keras.metrics.Precision(),tf.keras.metrics.Recall()])
    
    return model

model = cnn_1()
model.summary()

model2=cnn_2()
model2.summary()

training = model.fit(X_train, Y_train, epochs=5,verbose=1, validation_data=(X_test, Y_test))

results = model.evaluate(X_test, Y_test)
print('Test accuracy: ', results[1])

history_dict = training.history
#print(history_dict)
print(history_dict.keys())
plt.plot(training.history['accuracy'])
plt.plot(training.history['val_accuracy'])
plt.legend(['training', 'validation'], loc = 'upper left')
#plt.savefig('cnn2.png')
plt.show()

training = model2.fit(X_train, Y_train, epochs=30,verbose=1, validation_data=(X_test, Y_test))

results = model2.evaluate(X_test, Y_test)
print('Test accuracy: ', results[1])
print('Precision: ', results[2])
print('Recall: ', results[3])

history_dict = training.history
print(history_dict.keys())
plt.plot(training.history['accuracy'])
plt.plot(training.history['val_accuracy'])
plt.legend(['training', 'validation'], loc = 'upper left')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
#plt.savefig('cnn2.png')
plt.show()
plt.plot(training.history['precision_1'])
plt.plot(training.history['val_precision_1'])
plt.legend(['training', 'validation'], loc = 'upper left')
plt.title('Model Precision')
plt.ylabel('Precision')
plt.xlabel('Epoch')
#plt.savefig('cnn2.png')
plt.show()
plt.plot(training.history['recall_1'])
plt.plot(training.history['val_recall_1'])
plt.legend(['training', 'validation'], loc = 'upper left')
plt.title('Model Recall')
plt.ylabel('Recall')
plt.xlabel('epoch')
#plt.savefig('cnn2.png')
plt.show()

Y_test = np.argmax(Y_test, axis = 1)
print(Y_test)
j=0
ynew = model2.predict(X_test)
for i in ynew:
  print(i[Y_test[j]])
  j+=1
print(ynew)
Y_test = to_categorical(Y_test)

# Convert Keras model to TF Lite format and quantize.
converter = tf.lite.TFLiteConverter.from_keras_model(model2)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quantized_model = converter.convert()

# Save the quantized model to file to the Downloads directory
f = open('U.tflite', "wb")
f.write(tflite_quantized_model)
f.close()

# Download the digit classification model
from google.colab import files
files.download('U.tflite')

import pickle
FILENAME = '/content/drive/MyDrive/filename.pkl'
pickle.dump(model2, open(FILENAME, 'wb'))
#FILENAME = '/content/drive/MyDrive/filenamemodel.joblib'
#joblib.dump(model2, FILENAME)

model=pickle.load(open('/content/drive/MyDrive/filename.pkl','rb'))

from flask import Flask,Blueprint,request,render_template,jsonify
app=Flask(__name__)

@app.route('/predict',methods=['POST'])
def prediction():
  file=request.files['file']
  inputrate,inputdata = wavfile.read(f"{file}")
  rn=nr.reduce_noise(y = inputdata, sr=inputrate, n_std_thresh_stationary=1,stationary=True)
  reduced_noise = tf.cast(datachunk, dtype=tf.float32)
  spectrogram = tf.signal.stft(reduced_noise, frame_length=255, frame_step=128)
  spectrogram = tf.abs(spectrogram)
  spectogram=spectrogram.numpy()
  log_spec=np.log(spectrogram.T + np.finfo(float).eps)
  height = log_spec.shape[0]
  width = log_spec.shape[1]
  X=np.linspace(0, np.size(spectrogram), num=width, dtype=int)
  Y=range(height)
  spectogram_dimensions=(3,3)
  fig=plt.figure()
  fig.set_size_inches((spectogram_dimensions[0],spectogram_dimensions[1]))
  ax=plt.Axes(fig,[0.,0.,1.,1.])
  ax.pcolormesh(X, Y, log_spec)
  fig.add_axes(ax)
  label = file.split('_')[1][:-4]
  sample_number = file.split('_')[0]
  print(sample_number,label)  
  X_test = image.img_to_array(fig)
  y_test=label
  X_test=np.asanyarray(X_test)
  y_test=np.asanyarray(y_test)
  ynew = model.predict(X_test)
  return ynew[y_test[0]]